# SIPB Deep Learning Group
The schedule of readings for the SIPB/Cambridge AI Deep Learning Group If you have any papers you'd like to discuss, please either make a pull request, or send an email to the group and we'll add it. Papers with implementations available are strongly preferred.


## Suggested Papers:

* [Taskonomy: Disentangling Task Transfer Learning](https://arxiv.org/pdf/1804.08328.pdf)


## Schedule:


| Date  | Paper | Implementation |
| ------------- | ------------- |------------- |
|4.9.20|[MEMO: A Deep Network for Flexible Combination of Episodic Memories](https://arxiv.org/abs/2001.10913)||
|4.2.20|[Creating High Resolution Images with a Latent Adversarial Generator](https://arxiv.org/abs/2003.02365)||
|3.26.20|[Invertible Residual Networks](https://arxiv.org/abs/1811.00995)||
|3.5.20|[Value-driven Hindsight Modelling](https://arxiv.org/abs/2002.08329)||
|2.27.20|[Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958)||
|2.13.20|[Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365)||
|2.6.20|[Automated curricula through setter-solver interactions](https://arxiv.org/abs/1909.12892)||
|1.30.20|[Protein structure prediction ...](https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25834)|[deepmind](https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13)|
|1.23.20|[Putting An End to End-to-End: Gradient-Isolated Learning of Representations](https://arxiv.org/abs/1905.11786)||
|1.16.20|[Normalizing Flows: An Introduction and Review of Current Methods](https://arxiv.org/abs/1908.09257)||
|12.19.19|[Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265)||
|12.5.19|[On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)||
|Filling backwards (from April 9 down)|||
| ------------- | ------------- |------------- |
|8.21.18|[Universal Transformers](https://arxiv.org/abs/1807.03819)|[tensorflow](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/universal_transformer.py)|
|8.14.18|[Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)|[gautam1858](https://github.com/gautam1858/Neural-Arithmetic-Logic-Units)|
|8.7.18|[Neural Scene Representation and Rendering](https://deepmind.com/documents/211/Neural_Scene_Representation_and_Rendering_preprint.pdf)||
|7.31.18|[Measuring Abstract Reasoning in Neural Networks](https://arxiv.org/abs/1807.04225)||
|6.26.18|[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)|[openai](https://github.com/openai/finetune-transformer-lm)|
|6.19.18|[Associative Compression Networks for Representation Learning](https://arxiv.org/abs/1804.02476)||
|6.12.18|[On Characterizing the Capacity of Neural Networks using Algebraic Topology](https://arxiv.org/pdf/1802.04443.pdf)||
|6.5.18|[Causal Effect Inference with Deep Latent-Variable Models](https://arxiv.org/abs/1705.08821)|[AMLab](https://github.com/AMLab-Amsterdam/CEVAE)|
|5.29.18|[ML beyond Curve Fitting](http://www.inference.vc/untitled/)||
|5.22.18|[Synthesizing Programs for Images using Reinforced Adversarial Learning](https://arxiv.org/abs/1804.01118)||
|5.15.18|TensorFlow Overview|[r1.8](https://github.com/tensorflow/tensorflow/tree/r1.8)|
|5.8.18|[Compositional Attention Networks for Machine Reasoning](https://arxiv.org/abs/1803.03067)|[stanfordnlp](https://github.com/stanfordnlp/mac-network)|
|4.24.18|[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)||
|4.3.18|[How Developers Iterate on Machine Learning Workflows](https://arxiv.org/abs/1803.10311)||
|3.27.18|[Faster R-CNN: Towards Real-Time Object,Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)||
|3.20.18|[Attention Is All You Need](https://arxiv.org/abs/1706.03762)|[tensor2tensor](https://github.com/tensorflow/tensor2tensor)|
|3.6.18|[Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/abs/1801.10198)|[wikisum](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum), per [this gist](https://gist.github.com/peterjliu/f0dc9152a630520dc604c783db963aa7)|
|2.27.18|[AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks](https://arxiv.org/abs/1711.10485)|[StackGAN-v2](https://github.com/hanzhanggit/StackGAN-v2)|
|2.20.18|[Information Dropout](https://arxiv.org/pdf/1611.01353.pdf)|[InformationDropout](https://github.com/coventry/InformationDropout/blob/master/information_dropout.py), [official implementation](https://github.com/ucla-vision/information-dropout)|
|2.13.18|[Nested LSTMs](https://arxiv.org/abs/1801.10308)|[Nested-LSTM](https://github.com/titu1994/Nested-LSTM)|
|2.6.18|[Deep vs. Shallow Networks: An Approximation Theory Perspective](https://arxiv.org/abs/1608.03287)|
|1.30.18|[The Case for Learned Index Structures](https://arxiv.org/abs/1712.01208)|
|1.23.18|[Visualizing The Loss Landscape Of Neural Nets](https://arxiv.org/pdf/1712.09913.pdf)|
|1.16.18|[Go for a Walk and Arrive at the Answer](https://arxiv.org/abs/1711.05851), [RelNet: End-to-End Modeling of Entities & Relations](https://arxiv.org/abs/1706.07179)|
|1.9.18|[Intro to Coq](http://adam.chlipala.net/cpdt/) |
|12.12.17|[Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks](https://arxiv.org/abs/1607.01426) | [(ChainsofReasoning)](https://rajarshd.github.io/ChainsofReasoning/)
|12.5.17|[Stochastic Neural Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1704.03012)|[snn4hrl](https://github.com/florensacc/snn4hrl)|
|11.28.17|[Emergent Complexity via Multi-Agent Competition](https://arxiv.org/abs/1710.03748) [(blog post)](https://blog.openai.com/competitive-self-play/?)|[multiagent-competition](https://github.com/openai/multiagent-competition)|
|11.14.17|[Mastering the game of Go without human knowledge](https://deepmind.com/documents/119/agz_unformatted_nature.pdf) ||
|11.7.17|[Meta-Learning with Memory-Augmented Neural Networks](http://proceedings.mlr.press/v48/santoro16.html)|[ntm-meta-learning](https://github.com/ywatanabex/ntm-meta-learning)|
|10.24.17|[Poincar√© Embeddings for Learning Hierarchical Representations](https://arxiv.org/abs/1705.08039)|[poincare_embeddings](https://github.com/nishnik/poincare_embeddings)|
|10.17.17|[What does Attention in Neural Machine Translation Pay Attention to?](https://arxiv.org/abs/1710.03348v1)||
|10.10.17|[Zero-Shot Learning Through Cross-Modal Transfer](http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer.pdf)|[zslearning](https://github.com/mganjoo/zslearning)|
|9.26.17|[Variational Boosting: Iteratively Refining Posterior Approximations](https://arxiv.org/abs/1611.06585)|[vboost](https://github.com/andymiller/vboost)|
|9.19.17|[Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)|[cbfinn](https://github.com/cbfinn)|
|9.12.17|[Neuroscience-inspired AI](http://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3)||
|9.5.17|[Recurrent Dropout Without Memory Loss](https://arxiv.org/abs/1603.05118)|[rnn_cell_mulint_modern.py](https://github.com/NickShahML/tensorflow_with_latest_papers/blob/master/rnn_cell_mulint_modern.py#L141)|
|8.29.17|[Deep Transfer Learning with Joint Adaptation Networks](https://arxiv.org/abs/1605.06636)|jmmd.{[cpp](https://github.com/thuml/transfer-caffe/blob/41455ac37f11c18fb19509c93b381bcd51ded68e/src/caffe/layers/jmmd_layer.cpp),[hpp](https://github.com/thuml/transfer-caffe/blob/41455ac37f11c18fb19509c93b381bcd51ded68e/src/caffe/layers/jmmd_layer.cpp)}|
|8.22.17|[Designing Neural Network Architectures using Reinforcement Learning](https://arxiv.org/abs/1611.02167)|[metaqnn](https://github.com/bowenbaker/metaqnn)|
|8.15.17|[Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences](https://arxiv.org/abs/1610.09513)|[plstm](https://github.com/dannyneil/public_plstm)|
|8.8.17|[Hyper Networks](https://arxiv.org/abs/1609.09106)|[otoro blog](http://blog.otoro.net/2016/09/28/hyper-networks/)|
|8.1.17|[Full-Capacity Unitary Recurrent Neural Networks](https://arxiv.org/pdf/1611.00035.pdf)|[complex_RNN](https://github.com/amarshah/complex_RNN), [urnn](https://github.com/stwisdom/urnn)|
|7.25.17|[Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343) & [follow-up](https://arxiv.org/abs/1703.00522)|[dni.pytorch](https://github.com/andrewliao11/dni.pytorch)|
|7.18.17|[A simple neural network module for relational reasoning](https://arxiv.org/abs/1706.01427)|[relation-network](https://github.com/Alan-Lee123/relation-network)|
|7.11.17|[Speaker diarization using deep neural network embeddings](http://www.danielpovey.com/files/2017_icassp_diarization_embeddings.pdf)||
|6.20.17|[Neural Episodic Control](https://arxiv.org/abs/1703.01988)|[PFCM](https://github.com/PFCM/neural-episodic-control)|
|6.13.17|[Lie-Access Neural Turing Machines](https://arxiv.org/abs/1611.02854)|[harvardnlp](https://github.com/harvardnlp/lie-access-memory)|
|6.6.17|[Artistic style transfer for videos](https://arxiv.org/abs/1604.08610)|[artistic video](https://github.com/manuelruder/artistic-videos)|
|5.30.17|[High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)|[modular_rl](https://github.com/joschu/modular_rl)|
|5.23.17|[Emergence of Grounded Compositional Language in Multi-Agent Populations](https://arxiv.org/abs/1703.04908)||
|5.16.17|[Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)|[modular_rl](https://github.com/joschu/modular_rl)|
|5.9.17|[Improved Training of Wasserstein GANs](https://arxiv.org/abs/1704.00028)|[code](https://github.com/igul222/improved_wgan_training)|
|5.4.17|[Using Fast Weights to Attend to the Recent Past](https://arxiv.org/abs/1610.06258)||
|4.25.17|[Strategic Attentive Writer for Learning Macro-Actions](https://arxiv.org/abs/1606.04695)||
|4.18.17|[Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906)||
|4.4.17|[End to End Learning for Self-Driving Cars](https://arxiv.org/abs/1604.07316)||
|3.28.17|[Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning](https://arxiv.org/abs/1509.08731)||
|3.21.17|[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)||
|3.7.17|[Neural Programmer Interpreters](https://arxiv.org/abs/1511.06279)||
|2.14.17|[Wasserstein GAN](https://arxiv.org/abs/1701.07875)||
|2.7.17|[Towards Principled Methods for Training GANs](https://arxiv.org/abs/1701.04862)||
|1.31.17|[Mastering the Game of Go with Deep Networks](http://airesearch.com/wp-content/uploads/2016/01/deepmind-mastering-go.pdf)||
|1.24.17|[Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/abs/1611.03530)||
|1.17.17|[Neural Semantic Encoders](https://arxiv.org/abs/1607.04315)||
|12.21.16|[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.03242/)||
|12.14.16|[Key-Value Memory Networks for Directly Reading Documents](https://arxiv.org/abs/1606.03126)||
| 12.7.16  | [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657)  |
